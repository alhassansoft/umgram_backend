// src/search/diarySearch.ts
import { es } from "../lib/es";
import { DIARY_INDEX } from "./diaryIndex";
import { embedText } from "../services/embeddings";
import { expandQuery, DEFAULT_LLM_MODEL } from "../services/keywordExtractor"; // returns ClauseGraphPayload (+ optional en_simple)
import { logQueryExpansion } from "../services/queryLog";   // analysis-only logging
import { searchChunkedDiaries } from "../services/diaryChunkedIndexing";

type SearchHit<T = any> = {
  _id: string;
  _score: number;
  _source: T;
  highlight?: Record<string, string[]>;
};

// -------------------- helpers (typed for strings) --------------------
const toStr = (s?: unknown) => (s ?? "").toString().trim();
const low  = (s?: string) => toStr(s).toLowerCase();

const ensureStrArray = (x: unknown): string[] =>
  Array.isArray(x) ? (x as unknown[]).map((v) => toStr(v)).filter(Boolean) : [];

const uniqStr = (arr: ReadonlyArray<string>): string[] => Array.from(new Set(arr));

/** Tokenize a string to lowercase word tokens (ASCII letters + apostrophes) */
function getWordTokens(s: string): string[] {
  return (s.toLowerCase().match(/[a-z']+/g) ?? []).filter(Boolean);
}

/** Load English stopwords from env STOPWORDS_EN (JSON array of strings) */
function getStopwordSet(): Set<string> {
  const raw = process.env.STOPWORDS_EN;
  if (!raw) return new Set<string>();
  try {
    const arr = JSON.parse(raw);
    if (Array.isArray(arr)) {
      const out = new Set<string>();
      for (const v of arr) if (typeof v === "string") out.add(v.toLowerCase());
      return out;
    }
  } catch {
    // ignore invalid JSON
  }
  return new Set<string>();
}

/** Load misspellings map from environment (JSON). Example: {"futbal":"football","dont":"don't"} */
function getMisspellingsMap(): Record<string, string> {
  const raw = process.env.MISSPELLINGS_MAP;
  if (!raw) return {};
  try {
    const parsed = JSON.parse(raw);
    if (parsed && typeof parsed === "object") {
      const out: Record<string, string> = {};
      for (const [k, v] of Object.entries(parsed as Record<string, unknown>)) {
        if (typeof k === "string" && typeof v === "string") {
          out[k.toLowerCase()] = v.toLowerCase();
        }
      }
      return out;
    }
  } catch {
    // ignore invalid JSON and fall back to empty map
  }
  return {};
}

/** Detect negation (EN; supports curly/straight apostrophes) */
function isNegatedText(q: string): boolean {
  const negEN =
    /\b(?:not|no|never|didn[’']t|don[’']t|doesn[’']t|without|failed?\s+to|avoid(?:ed|ing)?|miss(?:ed|ing)?)\b/i;
  return negEN.test(q);
}

function hasOverlap(a: Set<string>, b: Set<string>): boolean {
  for (const v of a) if (b.has(v)) return true;
  return false;
}

/** Minimal common misspelling normalization for high-impact tokens */
function getNormalizedTokens(q: string): string[] {
  const misspellingsMap = getMisspellingsMap();
  const toks = (q.toLowerCase().match(/[a-z']+/g) ?? []).filter(Boolean);
  const out = new Set<string>();
  for (const t of toks) {
    const norm = misspellingsMap[t];
    if (norm) out.add(norm);
  }
  return Array.from(out);
}

/** Compact fallback text for fuzzy multi_match */
function buildFallbackQuery(
  userQuery: string,
  spans: string[],
  actions: string[],
  entities: string[],
  extraPhrases: string[]
): string {
  const parts = [
    userQuery?.trim() || "",
    ...spans,
    ...actions,
    ...entities,
    ...extraPhrases,
  ]
    .map((s) => (s || "").trim())
    .filter(Boolean);
  return uniqStr(parts).slice(0, 24).join(" ");
}

/** Surface action tokens (lemma or surface) from event clauses */
function getActionTokensFromClauses(payload: Awaited<ReturnType<typeof expandQuery>>): string[] {
  const out: string[] = [];
  for (const c of (payload as any).clauses ?? []) {
    if (c?.kind !== "event") continue;
    out.push(low((c.verb?.lemma || c.verb?.surface) as string));
  }
  return uniqStr(out).filter(Boolean);
}

/** Extract infinitive verbs from control-verb patterns like "to <verb>" within objects/span */
function getInfinitivesFromControlVerbs(payload: Awaited<ReturnType<typeof expandQuery>>): string[] {
  const CONTROL = new Set([
  "want", "try", "plan", "decide", "hope", "need", "like", "dislike", "avoid", "refuse", "intend", "promise", "agree", "prefer"
  ]);
  const toInf = (s?: string) => (s ?? "").toString().toLowerCase().trim();
  const out = new Set<string>();
  for (const c of (payload as any).clauses ?? []) {
    if (c?.kind !== "event") continue;
    const lemma = toInf(c.verb?.lemma || c.verb?.surface);
    if (!lemma || !CONTROL.has(lemma)) continue;
    // scan objects for "to <verb>"
    for (const o of (c.objects ?? [])) {
      const s = toInf(o);
      const m = s.match(/\bto\s+([a-z]+)\b/);
      if (m && m[1]) out.add(m[1]);
    }
    // scan source span as fallback
    const span = toInf((c as any).source_span);
    const m2 = span.match(/\bto\s+([a-z]+)\b/);
    if (m2 && m2[1]) out.add(m2[1]);
  }
  return Array.from(out);
}

/** Exact source spans from event clauses */
function getEventSpans(payload: Awaited<ReturnType<typeof expandQuery>>): string[] {
  const out: string[] = [];
  for (const c of (payload as any).clauses ?? []) {
    if (c?.kind !== "event") continue;
    const s = toStr((c as any).source_span);
    if (s) out.push(s);
  }
  return uniqStr(out);
}

/** Majority tense across events → coarse payload time */
function selectTimeLabel(payload: Awaited<ReturnType<typeof expandQuery>>):
  "past" | "present" | "future" | "unspecified" {
  const counts: Record<"past" | "present" | "future" | "unspecified", number> =
    { past: 0, present: 0, future: 0, unspecified: 0 };
  for (const c of (payload as any).clauses ?? []) {
    if (c?.kind !== "event") continue;
    const t = (c.verb?.tense || "unspecified") as keyof typeof counts;
    counts[t] = (counts[t] ?? 0) + 1;
  }
  const order: Array<keyof typeof counts> = ["past", "present", "future"];
  let best: keyof typeof counts = "unspecified";
  let bestCnt = 0;
  for (const k of order) {
    if (counts[k] > bestCnt) {
      best = k; bestCnt = counts[k];
    }
  }
  return bestCnt ? best : "unspecified";
}

/** Overall polarity from event-level negation flags (fallback to text) */
function selectPolarity(
  payload: Awaited<ReturnType<typeof expandQuery>>,
  userQuery: string
): "affirmative" | "negative" {
  let neg = 0, pos = 0;
  for (const c of (payload as any).clauses ?? []) {
    if (c?.kind !== "event") continue;
    if (c.verb?.negation) neg++; else pos++;
  }
  if (neg > 0 && pos === 0) return "negative";
  if (pos > 0 && neg === 0) return "affirmative";
  return isNegatedText(userQuery) ? "negative" : "affirmative";
}

/** Pull EN-simple expansions (typed) if available */
function fromEnSimple(payload: any): {
  entities: string[];
  actions: string[];
  phrases: string[];
  entitySynonyms: string[];
  actionSynonyms: string[];
  paraphrases: string[];
} {
  const en = (payload?.en_simple ?? {}) as any;

  const entities = ensureStrArray(en.entities).map(low);
  const actions  = ensureStrArray(en.actions).map(low);
  const phrases  = ensureStrArray(en.phrases_en).map(toStr);
  const paraphrases = ensureStrArray(en.paraphrases).map(toStr);

  const entitySynsSet = new Set<string>();
  if (Array.isArray(en?.synsets?.entity_synsets)) {
    for (const e of en.synsets.entity_synsets) {
      const lemma = low((e?.lemma as string) ?? "");
      if (lemma) entitySynsSet.add(lemma);
      for (const s of ensureStrArray(e?.synonyms)) entitySynsSet.add(low(s));
    }
  }

  const actionSynsSet = new Set<string>();
  if (Array.isArray(en?.synsets?.action_synsets)) {
    for (const a of en.synsets.action_synsets) {
      const lemma = low((a?.lemma as string) ?? "");
      if (lemma) actionSynsSet.add(lemma);
      for (const s of ensureStrArray(a?.synonyms)) actionSynsSet.add(low(s));
    }
  }

  return {
    entities: uniqStr(entities),
    actions: uniqStr(actions),
    phrases: uniqStr(phrases),
    entitySynonyms: Array.from(entitySynsSet),
  actionSynonyms: Array.from(actionSynsSet),
  paraphrases: uniqStr(paraphrases),
  };
}

/** Main ES body (hybrid: kNN + lexical, English-only, w/ fuzziness) */
function buildMainBody(args: {
  qVec: number[];
  entitiesQ: string[];
  actionsQ: string[];
  spansQ: string[];
  paraQ: string[];
  wantPol: "affirmative" | "negative";
  payloadTime: "past" | "present" | "future" | "unspecified";
  knnFilterMust: any[];
  fallbackText: string;
  rawQuery: string;
}) {
  const {
    qVec, entitiesQ, actionsQ, spansQ, paraQ, wantPol, payloadTime, knnFilterMust, fallbackText, rawQuery,
  } = args;

  const must: any[] = [];
  if (entitiesQ.length) {
    must.push({
      bool: {
        should: [
          { terms: { entities: entitiesQ } },
          ...entitiesQ.map((e) => ({ match_phrase: { "content":    { query: e } } })),
          ...entitiesQ.map((e) => ({ match_phrase: { "title":      { query: e } } })),
          ...entitiesQ.map((e) => ({ match_phrase: { "phrases_en": { query: e } } })),
        ],
        minimum_should_match: 1,
      },
    });
  }

  const should: any[] = [];

  if (actionsQ.length) {
    should.push({ terms: { actions: actionsQ,      boost: 2.2 } });
    should.push({ terms: { sensitive_en: actionsQ, boost: 1.8 } });

    for (const a of actionsQ) {
      should.push({ match_phrase: { "content": { query: a, boost: 2.2 } } });
      should.push({ match_phrase: { "title":   { query: a, boost: 1.8 } } });
    }

    if (wantPol === "negative") {
      should.push({ terms: { negated_actions_en: actionsQ,  boost: 2.0 } as any });
    } else {
      should.push({ terms: { affirmed_actions_en: actionsQ, boost: 2.0 } as any });
    }
  }

  for (const p of spansQ) {
    should.push({ match_phrase: { "content":   { query: p, boost: 2.6 } } });
    should.push({ match_phrase: { "phrases_en":{ query: p, boost: 1.6 } } });
  }

  // Add a light-weight paraphrase boost against content/title; keep it modest and capped
  for (const p of paraQ.slice(0, 5)) {
    should.push({ match_phrase: { "content": { query: p, boost: 1.2 } } });
    should.push({ match_phrase: { "title":   { query: p, boost: 0.8 } } });
  }

  if (payloadTime !== "unspecified") {
    should.push({ term: { time_label: { value: payloadTime, boost: 0.2 } } });
  }
  should.push({ term: { polarity: { value: wantPol, boost: 0.3 } } });

  if (fallbackText) {
    should.push({
      multi_match: {
        query: fallbackText,
        fields: [
          "title.std^1.8", "content.std^1.6",
          "title.ascii^1.8","content.ascii^1.6",
          "phrases_en^1.2",
          "inquiry_en^1.2"
        ],
        type: "best_fields",
        operator: "OR",
        fuzziness: "AUTO",
        minimum_should_match: "30%",
      },
    } as any);
  }

  // Direct fuzzy match on the original raw query to catch typos like 'futbal'
  if (rawQuery?.trim()) {
    should.push({ match: { "content.std":  { query: rawQuery, fuzziness: "AUTO", minimum_should_match: "15%", boost: 0.8 } } });
    should.push({ match: { "content.ascii":{ query: rawQuery, fuzziness: "AUTO", minimum_should_match: "15%", boost: 0.6 } } });
  should.push({ match: { "inquiry_en":   { query: rawQuery, fuzziness: "AUTO", minimum_should_match: "15%", boost: 0.6 } } });
  }

  for (const e of entitiesQ) {
    should.push({ match: { "content.std":  { query: e, fuzziness: "AUTO", boost: 1.6 } } });
    should.push({ match: { "content.ascii":{ query: e, fuzziness: "AUTO", boost: 1.4 } } });
    should.push({ match: { "title.std":    { query: e, fuzziness: "AUTO", boost: 1.4 } } });
  should.push({ match: { "inquiry_en":   { query: e, fuzziness: "AUTO", boost: 1.2 } } });
  }
  for (const a of actionsQ) {
    should.push({ match: { "content.std":  { query: a, fuzziness: "AUTO", boost: 1.6 } } });
    should.push({ match: { "content.ascii":{ query: a, fuzziness: "AUTO", boost: 1.4 } } });
    should.push({ match: { "title.std":    { query: a, fuzziness: "AUTO", boost: 1.4 } } });
  should.push({ match: { "inquiry_en":   { query: a, fuzziness: "AUTO", boost: 1.2 } } });
  }

  return {
    min_score: 0.15,

    knn: {
      field: "vec",
      query_vector: qVec,
      k: 100,
      num_candidates: 1000,
      ...(knnFilterMust.length ? { filter: { bool: { must: knnFilterMust } } } : {}),
    },

    query: {
      bool: {
        must,
        should,
        minimum_should_match: should.length > 0 ? 1 : 0,
      },
    },

    _source: [
      "title",
      "content",
  "userId",
      "entities",
      "actions",
      "sensitive_en",
      "phrases_en",
      "time_label",
      "polarity",
      "updatedAt",
      "negated_actions_en",
      "affirmed_actions_en",
    ],

    highlight: {
      pre_tags: ["<mark>"],
      post_tags: ["</mark>"],
      fields: {
        title: { number_of_fragments: 0 },
        content: { fragment_size: 120, number_of_fragments: 3 },
        phrases_en: { fragment_size: 40, number_of_fragments: 5 },
      },
      require_field_match: false,
    },

    size: 20,
  };
}

/** Pure lexical fallback (no kNN, forgiving, with fuzziness) */
function buildLexicalFallbackBody(args: {
  entitiesQ: string[];
  actionsQ: string[];
  spansQ: string[];
  paraQ: string[];
  wantPol: "affirmative" | "negative";
  payloadTime: "past" | "present" | "future" | "unspecified";
  fallbackText: string;
  rawQuery: string;
}) {
  const { entitiesQ, actionsQ, spansQ, paraQ, wantPol, payloadTime, fallbackText, rawQuery } = args;

  const must: any[] = [];
  if (entitiesQ.length) {
    must.push({
      bool: {
        should: [
          { terms: { entities: entitiesQ } },
          ...entitiesQ.map((e) => ({ match: { "content.std":   { query: e, fuzziness: "AUTO" } } })),
          ...entitiesQ.map((e) => ({ match: { "content.ascii": { query: e, fuzziness: "AUTO" } } })),
          ...entitiesQ.map((e) => ({ match: { "title.std":     { query: e, fuzziness: "AUTO" } } })),
        ],
        minimum_should_match: 1,
      },
    });
  }

  const should: any[] = [];
  if (actionsQ.length) {
    should.push({ terms: { actions: actionsQ,      boost: 2.0 } });
    should.push({ terms: { sensitive_en: actionsQ, boost: 1.6 } });

    for (const a of actionsQ) {
      should.push({ match_phrase: { "content":    { query: a, boost: 2.0 } } });
      should.push({ match:        { "content.std":   { query: a, fuzziness: "AUTO", boost: 1.8 } } });
      should.push({ match:        { "content.ascii": { query: a, fuzziness: "AUTO", boost: 1.6 } } });
      should.push({ match:        { "title.std":     { query: a, fuzziness: "AUTO", boost: 1.4 } } });
    }
  }

  for (const p of spansQ) {
    should.push({ match_phrase: { "content":    { query: p, boost: 2.0 } } });
    should.push({ match_phrase: { "phrases_en": { query: p, boost: 1.4 } } });
  }

  for (const p of paraQ.slice(0, 5)) {
    should.push({ match_phrase: { "content": { query: p, boost: 1.0 } } });
    should.push({ match_phrase: { "title":   { query: p, boost: 0.6 } } });
  }

  if (payloadTime !== "unspecified") {
    should.push({ term: { time_label: { value: payloadTime, boost: 0.2 } } });
  }
  should.push({ term: { polarity: { value: wantPol, boost: 0.3 } } });

  if (fallbackText) {
    should.push({
      multi_match: {
        query: fallbackText,
        fields: [
          "title.std^1.6", "content.std^1.4",
          "title.ascii^1.6","content.ascii^1.4",
          "phrases_en^1.2",
          "inquiry_en^1.2"
        ],
        type: "best_fields",
        operator: "OR",
        fuzziness: "AUTO",
        minimum_should_match: "20%",
      },
    } as any);
  }

  // Direct fuzzy match on the original query as a catch-all in lexical fallback
  if (rawQuery?.trim()) {
    should.push({ match: { "content.std":  { query: rawQuery, fuzziness: "AUTO", minimum_should_match: "10%", boost: 0.8 } } });
    should.push({ match: { "content.ascii":{ query: rawQuery, fuzziness: "AUTO", minimum_should_match: "10%", boost: 0.6 } } });
  should.push({ match: { "inquiry_en":   { query: rawQuery, fuzziness: "AUTO", minimum_should_match: "10%", boost: 0.6 } } });
  }

  return {
    min_score: 0.0,
    query: {
      bool: {
        must,
        should,
        minimum_should_match: 0,
      },
    },
    _source: [
      "title",
      "content",
  "userId",
      "entities",
      "actions",
      "sensitive_en",
      "phrases_en",
      "time_label",
      "polarity",
      "updatedAt",
      "negated_actions_en",
      "affirmed_actions_en",
    ],
    highlight: {
      pre_tags: ["<mark>"],
      post_tags: ["</mark>"],
      fields: {
        title: { number_of_fragments: 0 },
        content: { fragment_size: 120, number_of_fragments: 3 },
        phrases_en: { fragment_size: 40, number_of_fragments: 5 },
      },
      require_field_match: false,
    },
    size: 20,
  };
}

/**
 * Primary search function using chunked indexing
 */
export async function searchDiariesSemanticChunked(
  userQuery: string,
  opts: {
    userId?: string;
    mode?: "wide" | "strict";
    scope?: "mine" | "others" | "all";
    method?: "normal" | "vector" | "hybrid";
    size?: number;
    from?: number;
  } = {}
): Promise<SearchHit[]> {
  if (!es) return [];

  const { 
    userId = "", 
    mode = "wide", 
    scope = "all", 
    method = "normal",
    size = 20,
    from = 0
  } = opts;

  if (!userQuery.trim()) return [];

  try {
    // Build base query filters
    const filters: any[] = [];
    
    if (scope === "mine" && userId) {
      filters.push({ term: { userId } });
    } else if (scope === "others" && userId) {
      filters.push({
        bool: {
          must_not: [{ term: { userId } }]
        }
      });
    }

    let query: any;

    if (method === "vector") {
      // Pure vector search
      const vec = await embedText(userQuery);
      query = {
        bool: {
          must: [
            {
              knn: {
                field: "vec",
                query_vector: vec,
                k: size * 2,
                num_candidates: 100,
              }
            }
          ],
          filter: filters
        }
      };
    } else if (method === "hybrid") {
      // Hybrid approach combining text and vector
      const vec = await embedText(userQuery);
      query = {
        bool: {
          should: [
            {
              multi_match: {
                query: userQuery,
                fields: ["title^3", "content^2", "phrases_en", "inquiry_en"],
                type: "best_fields",
                fuzziness: "AUTO",
              }
            },
            {
              knn: {
                field: "vec",
                query_vector: vec,
                k: size,
                num_candidates: 50,
                boost: 0.8
              }
            }
          ],
          minimum_should_match: 1,
          filter: filters
        }
      };
    } else {
      // Normal text search with expanded query
      try {
        const expanded = await expandQuery(userQuery, {
          model: DEFAULT_LLM_MODEL,
          temperature: 0.1,
        });

        await logQueryExpansion({ 
          rawQuery: userQuery, 
          payload: expanded,
          userId,
          mode 
        });

        const actions = getActionTokensFromClauses(expanded);
        const entities = ensureStrArray((expanded as any).entities);
        const spans = getEventSpans(expanded);

        // Build comprehensive text query
        const textQuery = {
          bool: {
            should: [
              // Direct match on original query
              {
                multi_match: {
                  query: userQuery,
                  fields: ["title^3", "content^2"],
                  type: "best_fields",
                  fuzziness: "AUTO",
                  boost: 2.0
                }
              },
              // Match on extracted entities
              ...(entities.length > 0 ? [{
                terms: {
                  entities: entities,
                  boost: 1.5
                }
              }] : []),
              // Match on extracted actions
              ...(actions.length > 0 ? [{
                terms: {
                  actions: actions,
                  boost: 1.8
                }
              }] : []),
              // Match on phrases
              ...(spans.length > 0 ? [{
                multi_match: {
                  query: spans.join(" "),
                  fields: ["phrases_en", "inquiry_en"],
                  type: "phrase",
                  boost: 1.3
                }
              }] : []),
              // Fallback fuzzy search
              {
                multi_match: {
                  query: buildFallbackQuery(userQuery, spans, actions, entities, []),
                  fields: ["content", "title", "phrases_en"],
                  type: "cross_fields",
                  fuzziness: "AUTO",
                  boost: 0.8
                }
              }
            ],
            minimum_should_match: 1,
            filter: filters
          }
        };

        query = textQuery;
      } catch (err) {
        console.warn("[DiarySearch] Query expansion failed, falling back to simple search:", err);
        query = {
          bool: {
            must: [
              {
                multi_match: {
                  query: userQuery,
                  fields: ["title^3", "content^2", "phrases_en"],
                  type: "best_fields",
                  fuzziness: "AUTO",
                }
              }
            ],
            filter: filters
          }
        };
      }
    }

    // Use chunked search function
    const result = await searchChunkedDiaries(query, {
      size,
      from,
      aggregateChunks: true // Group chunks back into diaries
    });

    return result.hits;

  } catch (err) {
    console.error("[DiarySearch] Search failed:", err);
    return [];
  }
}

/**
 * Semantic + lexical diary search with chunking support
 */
export async function searchDiariesSemantic(
  userQuery: string,
  opts?: {
    mode?: "wide" | "strict";
    scope?: "mine" | "others" | "all";
    userId?: string | null;
    method?: "normal" | "vector" | "hybrid";
    size?: number;
    from?: number;
    logMeta?: { userId?: string | null; ip?: string | null; ua?: string | null } | null;
  }
): Promise<SearchHit[]> {
  // Use new chunked search system
  return searchDiariesSemanticChunked(userQuery, {
    userId: opts?.userId || "",
    mode: opts?.mode || "wide",
    scope: opts?.scope || "all",
    method: opts?.method || "normal",
    size: opts?.size || 20,
    from: opts?.from || 0,
  });
}

  const actionsFromClauses: string[] = getActionTokensFromClauses(payload).map(low);
  const actionsFromInfinitives: string[] = getInfinitivesFromControlVerbs(payload);
  const actionsQ: string[] = uniqStr([
    ...actionsFromClauses,
    ...actionsFromInfinitives,
    ...en.actions,
    ...en.actionSynonyms,
  ]);

  const spansQ: string[] = uniqStr([
    ...getEventSpans(payload),
    ...en.phrases,
  ]);

  const paraQ: string[] = uniqStr([...en.paraphrases]);
  const normQ: string[] = getNormalizedTokens(userQuery);
  const paraAll: string[] = uniqStr([...paraQ, ...normQ]);

  // Sensitive verbs: actions bag
  const sensitiveTerms: string[] = uniqStr([...actionsQ]); // reserved if needed later

  // Time/Polarity (derived from graph; fallback to text negation)
  const payloadTime = selectTimeLabel(payload);
  const wantPol = selectPolarity(payload, userQuery);

  // Optional kNN filter by entities
  const knnFilterMust: any[] = [];
  // Scope filter (mine, others, all)
  const docFilterMust: any[] = [];
  if (scope === "mine" && scopeUserId) {
    // only my diaries
    docFilterMust.push({ term: { userId: scopeUserId } });
  } else if (scope === "others" && scopeUserId) {
    // exclude my diaries
    docFilterMust.push({ bool: { must_not: { term: { userId: scopeUserId } } } });
  }
  // Normalize entity tokens to avoid over-filtering on typos/pronouns
  const PRONOUNS = new Set(["i","you","he","she","it","we","they","me","him","her","us","them"]);
  const misspellingsMap2 = getMisspellingsMap();
  const entsForFilter = entitiesQ
    .map((e) => misspellingsMap2[e] || e)
    .filter((e) => e && e.length >= 3 && !PRONOUNS.has(e));
  if (mode === "strict" && entsForFilter.length) {
    knnFilterMust.push({
      bool: {
        should: [{ terms: { entities: entsForFilter } }],
        minimum_should_match: 1,
      },
    });
  }
  // Apply scope filters to kNN filter as well
  if (docFilterMust.length) {
    knnFilterMust.push(...docFilterMust);
  }

  // 4) Hybrid search (kNN + lexical with fuzziness)
  const fallbackText = buildFallbackQuery(userQuery, spansQ, actionsQ, entitiesQ, [...en.phrases, ...paraAll]);

  const body = buildMainBody({
    qVec,
    entitiesQ,
    actionsQ,
    spansQ,
  paraQ: paraAll,
    wantPol,
    payloadTime,
    knnFilterMust,
  fallbackText,
  rawQuery: userQuery,
  });

  // Inject document-level filters (scope) into main query
  if ((body as any).query?.bool && docFilterMust.length) {
    const b = (body as any).query.bool;
    b.filter = Array.isArray(b.filter) ? [...b.filter, ...docFilterMust] : [...docFilterMust];
  }

  const res = await es.search<unknown>({
    index: DIARY_INDEX,
    ...(body as any),
  });

  let hits: SearchHit[] = (res as any).hits.hits as any;

  // --- Arabic / Mixed-language relevance boost (phase 1 re-rank) ---
  // Extract Arabic tokens from the raw user query and boost documents containing them.
  // This is a lightweight lexical layer to complement embeddings (currently English-oriented).
  try {
    const arTokens = Array.from(new Set((userQuery.match(/[\u0621-\u064A]+/g) || []).filter(t => t.length > 1)));
    if (arTokens.length) {
      const fullPhrase = userQuery.replace(/\s+/g, ' ').trim();
      for (const h of hits) {
        const src: any = h._source || {};
        const text = `${src.title || ''} ${src.content || ''}`.toLowerCase();
        let matches = 0;
        for (const t of arTokens) {
          if (text.includes(t.toLowerCase())) matches++;
        }
        if (matches > 0) {
          // Base token match boost (capped)
          const tokenBoost = Math.min(matches, 6) * 0.15; // up to +0.9
          h._score += tokenBoost;
        }
        // Phrase / co-occurrence boost if all tokens appear OR the contiguous phrase appears
        if (arTokens.every(t => text.includes(t.toLowerCase()))) {
          h._score += 0.25; // all-token presence bonus
        }
        if (fullPhrase && fullPhrase.length > 4 && text.includes(fullPhrase.toLowerCase())) {
          h._score += 0.35; // exact phrase bonus
        }
      }
      // Resort after boosting
      hits.sort((a,b) => b._score - a._score);
    }
  } catch {}

  // 5) Strict post-filter: prefer negation-aware buckets
  if (hits.length && mode === "strict") {
    const qSensitiveAll = new Set<string>(actionsQ.map(low).filter(Boolean));
    // Attribute-like tokens from query (e.g., adjectives)
    const qTokens = new Set<string>(getWordTokens(userQuery));
    for (const a of actionsQ) qTokens.delete(low(a));
    for (const e of entitiesQ) qTokens.delete(low(e));
    const stopset = getStopwordSet();
    if (stopset.size) for (const w of Array.from(qTokens)) if (stopset.has(w)) qTokens.delete(w);
    const qEntities = new Set<string>(entitiesQ.map(low).filter(Boolean));
    const negRe = /\b(?:not|no|never|don['’]?t|didn['’]?t|doesn['’]?t|without)\b/;

    hits = hits.filter((h) => {
      const src: any = h._source || {};

      const docPol = low(src.polarity || "");

      const docAffVerbBag = new Set<string>((src.affirmed_actions_en || []).map(low));
      const docNegVerbBag = new Set<string>((src.negated_actions_en  || []).map(low));
      const docVerbBag    = new Set<string>(
        ([...(src.actions || []), ...(src.sensitive_en || [])] as string[])
          .map(low)
          .filter(Boolean)
      );

  const hasAffOverlap = docAffVerbBag.size && hasOverlap(qSensitiveAll, docAffVerbBag);
  const hasNegOverlap = docNegVerbBag.size && hasOverlap(qSensitiveAll, docNegVerbBag);
  const hasAnyVerbOverlap = docVerbBag.size > 0 && hasOverlap(qSensitiveAll, docVerbBag);

      const docText = toStr((src.content as string) || (src.title as string) || "").toLowerCase();
      // Attribute gating near entity: if query is negative and attribute tokens appear near entity window without negation → exclude
      const qAttrTokens = new Set<string>(Array.from(qTokens));
      let attrHit = false;
      let attrNeg = false;
      if (qEntities.size > 0 && qAttrTokens.size) {
        for (const e of Array.from(qEntities)) {
          if (!docText.includes(e)) continue;
          const idx = docText.indexOf(e);
          const start = Math.max(0, idx - 64);
          const end = Math.min(docText.length, idx + e.length + 64);
          const win = docText.slice(start, end);
          for (const t of Array.from(qAttrTokens)) { if (t.length < 3) continue; if (win.includes(t)) { attrHit = true; break; } }
          if (negRe.test(win)) attrNeg = true;
          if (attrHit && attrNeg) break;
        }
      }
      if (wantPol === 'negative' && attrHit && !attrNeg) return false;

      // Polarity-aware verb logic
      if (qSensitiveAll.size > 0) {
        if (wantPol === 'affirmative') {
          if (hasAffOverlap || hasAnyVerbOverlap) return true;
          return false;
        } else {
          // negative: require negated overlap, or negation near entity + any verb overlap
          let negNearEntity = false;
          if (qEntities.size > 0) {
            for (const e of Array.from(qEntities)) {
              if (!docText.includes(e)) continue;
              const idx = docText.indexOf(e);
              const start = Math.max(0, idx - 64);
              const end = Math.min(docText.length, idx + e.length + 64);
              if (negRe.test(docText.slice(start, end))) { negNearEntity = true; break; }
            }
          }
          if (hasNegOverlap || (negNearEntity && hasAnyVerbOverlap)) return true;
          return false;
        }
      }

      // No explicit verbs in query: rely on doc polarity and negation presence
      if ((docPol === 'affirmative' || docPol === 'negative') && docPol !== wantPol) return false;
      if (wantPol === 'negative' && !negRe.test(docText)) return false;
      return true;
    });
  }

  // 6) Pure lexical fallback (no vector gate)
  if (hits.length === 0) {
    const lexBody = buildLexicalFallbackBody({
      entitiesQ,
      actionsQ,
      spansQ,
  paraQ: paraAll,
      wantPol,
      payloadTime,
  fallbackText,
  rawQuery: userQuery,
    });

  const res2 = await es.search<unknown>({
      index: DIARY_INDEX,
      ...(lexBody as any),
    });

  let hits2: SearchHit[] = (res2 as any).hits.hits as any;

    // Arabic / mixed-language boost for fallback path as well
    try {
      const arTokens2 = Array.from(new Set((userQuery.match(/[\u0621-\u064A]+/g) || []).filter(t => t.length > 1)));
      if (arTokens2.length) {
        const fullPhrase2 = userQuery.replace(/\s+/g, ' ').trim();
        for (const h of hits2) {
          const src: any = h._source || {};
          const text = `${src.title || ''} ${src.content || ''}`.toLowerCase();
          let matches = 0;
          for (const t of arTokens2) {
            if (text.includes(t.toLowerCase())) matches++;
          }
          if (matches > 0) h._score += Math.min(matches, 6) * 0.15;
          if (arTokens2.every(t => text.includes(t.toLowerCase()))) h._score += 0.25;
          if (fullPhrase2 && fullPhrase2.length > 4 && text.includes(fullPhrase2.toLowerCase())) h._score += 0.35;
        }
        hits2.sort((a,b)=> b._score - a._score);
      }
    } catch {}

    // Also enforce scope filters on lexical fallback results (post-filtering if needed)
    if (docFilterMust.length) {
      hits2 = hits2.filter((h: any) => {
        const src: any = h._source || {};
        if (scope === "mine" && scopeUserId) return String(src.userId || "") === scopeUserId;
        if (scope === "others" && scopeUserId) return String(src.userId || "") !== scopeUserId;
        return true;
      });
    }

    if (mode === "strict") {
      const qSensitiveAll = new Set<string>(actionsQ.map(low).filter(Boolean));
      const qTokens = new Set<string>(getWordTokens(userQuery));
      for (const a of actionsQ) qTokens.delete(low(a));
      for (const e of entitiesQ) qTokens.delete(low(e));
      const stopset = getStopwordSet();
      if (stopset.size) for (const w of Array.from(qTokens)) if (stopset.has(w)) qTokens.delete(w);
      const qEntities = new Set<string>(entitiesQ.map(low).filter(Boolean));
      const negRe = /\b(?:not|no|never|don['’]?t|didn['’]?t|doesn['’]?t|without)\b/;

      hits2 = hits2.filter((h) => {
        const src: any = h._source || {};

        const docPol = low(src.polarity || "");

        const docAffVerbBag = new Set<string>((src.affirmed_actions_en || []).map(low));
        const docNegVerbBag = new Set<string>((src.negated_actions_en  || []).map(low));
        const docVerbBag    = new Set<string>(
          ([...(src.actions || []), ...(src.sensitive_en || [])] as string[])
            .map(low)
            .filter(Boolean)
        );

  const hasAffOverlap = docAffVerbBag.size && hasOverlap(qSensitiveAll, docAffVerbBag);
  const hasNegOverlap = docNegVerbBag.size && hasOverlap(qSensitiveAll, docNegVerbBag);
  const hasAnyVerbOverlap = docVerbBag.size > 0 && hasOverlap(qSensitiveAll, docVerbBag);

        const docText = toStr((src.content as string) || (src.title as string) || "").toLowerCase();
        // Attribute gating near entity
        const qAttrTokens = new Set<string>(Array.from(qTokens));
        let attrHit = false;
        let attrNeg = false;
        if (qEntities.size > 0 && qAttrTokens.size) {
          for (const e of Array.from(qEntities)) {
            if (!docText.includes(e)) continue;
            const idx = docText.indexOf(e);
            const start = Math.max(0, idx - 64);
            const end = Math.min(docText.length, idx + e.length + 64);
            const win = docText.slice(start, end);
            for (const t of Array.from(qAttrTokens)) { if (t.length < 3) continue; if (win.includes(t)) { attrHit = true; break; } }
            if (negRe.test(win)) attrNeg = true;
            if (attrHit && attrNeg) break;
          }
        }
        if (wantPol === 'negative' && attrHit && !attrNeg) return false;

        if (qSensitiveAll.size > 0) {
          if (wantPol === 'affirmative') {
            if (hasAffOverlap || hasAnyVerbOverlap) return true;
            return false;
          } else {
            let negNearEntity = false;
            if (qEntities.size > 0) {
              for (const e of Array.from(qEntities)) {
                if (!docText.includes(e)) continue;
                const idx = docText.indexOf(e);
                const start = Math.max(0, idx - 64);
                const end = Math.min(docText.length, idx + e.length + 64);
                if (negRe.test(docText.slice(start, end))) { negNearEntity = true; break; }
              }
            }
            if (hasNegOverlap || (negNearEntity && hasAnyVerbOverlap)) return true;
            return false;
          }
        }

        if ((docPol === 'affirmative' || docPol === 'negative') && docPol !== wantPol) return false;
        if (wantPol === 'negative' && !negRe.test(docText)) return false;
        return true;
      });
    }

    return hits2;
  }

/**
 * Primary search function using chunked indexing
 */
export async function searchDiariesSemanticChunked(
  userQuery: string,
  opts: {
    userId?: string;
    mode?: "wide" | "strict";
    scope?: "mine" | "others" | "all";
    method?: "normal" | "vector" | "hybrid";
    size?: number;
    from?: number;
  } = {}
): Promise<SearchHit[]> {
  if (!es) return [];

  const { 
    userId = "", 
    mode = "wide", 
    scope = "all", 
    method = "normal",
    size = 20,
    from = 0
  } = opts;

  if (!userQuery.trim()) return [];

  try {
    // Build base query filters
    const filters: any[] = [];
    
    if (scope === "mine" && userId) {
      filters.push({ term: { userId } });
    } else if (scope === "others" && userId) {
      filters.push({
        bool: {
          must_not: [{ term: { userId } }]
        }
      });
    }

    let query: any;

    if (method === "vector") {
      // Pure vector search
      const vec = await embedText(userQuery);
      query = {
        bool: {
          must: [
            {
              knn: {
                field: "vec",
                query_vector: vec,
                k: size * 2,
                num_candidates: 100,
              }
            }
          ],
          filter: filters
        }
      };
    } else if (method === "hybrid") {
      // Hybrid approach combining text and vector
      const vec = await embedText(userQuery);
      query = {
        bool: {
          should: [
            {
              multi_match: {
                query: userQuery,
                fields: ["title^3", "content^2", "phrases_en", "inquiry_en"],
                type: "best_fields",
                fuzziness: "AUTO",
              }
            },
            {
              knn: {
                field: "vec",
                query_vector: vec,
                k: size,
                num_candidates: 50,
                boost: 0.8
              }
            }
          ],
          minimum_should_match: 1,
          filter: filters
        }
      };
    } else {
      // Normal text search with expanded query
      try {
        const expanded = await expandQuery(userQuery, {
          model: DEFAULT_LLM_MODEL,
          temperature: 0.1,
        });

        await logQueryExpansion({ 
          rawQuery: userQuery, 
          payload: expanded,
          userId,
          mode 
        });

        const actions = getActionTokensFromClauses(expanded);
        const entities = ensureStrArray((expanded as any).entities);
        const spans = getEventSpans(expanded);

        // Build comprehensive text query
        const textQuery = {
          bool: {
            should: [
              // Direct match on original query
              {
                multi_match: {
                  query: userQuery,
                  fields: ["title^3", "content^2"],
                  type: "best_fields",
                  fuzziness: "AUTO",
                  boost: 2.0
                }
              },
              // Match on extracted entities
              ...(entities.length > 0 ? [{
                terms: {
                  entities: entities,
                  boost: 1.5
                }
              }] : []),
              // Match on extracted actions
              ...(actions.length > 0 ? [{
                terms: {
                  actions: actions,
                  boost: 1.8
                }
              }] : []),
              // Match on phrases
              ...(spans.length > 0 ? [{
                multi_match: {
                  query: spans.join(" "),
                  fields: ["phrases_en", "inquiry_en"],
                  type: "phrase",
                  boost: 1.3
                }
              }] : []),
              // Fallback fuzzy search
              {
                multi_match: {
                  query: buildFallbackQuery(userQuery, spans, actions, entities, []),
                  fields: ["content", "title", "phrases_en"],
                  type: "cross_fields",
                  fuzziness: "AUTO",
                  boost: 0.8
                }
              }
            ],
            minimum_should_match: 1,
            filter: filters
          }
        };

        query = textQuery;
      } catch (err) {
        console.warn("[DiarySearch] Query expansion failed, falling back to simple search:", err);
        query = {
          bool: {
            must: [
              {
                multi_match: {
                  query: userQuery,
                  fields: ["title^3", "content^2", "phrases_en"],
                  type: "best_fields",
                  fuzziness: "AUTO",
                }
              }
            ],
            filter: filters
          }
        };
      }
    }

    // Use chunked search function
    const result = await searchChunkedDiaries(query, {
      size,
      from,
      aggregateChunks: true // Group chunks back into diaries
    });

    return result.hits;

  } catch (err) {
    console.error("[DiarySearch] Search failed:", err);
    return [];
  }
}

/**
 * Semantic + lexical diary search with chunking support
 */
export async function searchDiariesSemantic(
  userQuery: string,
  opts?: {
    mode?: "wide" | "strict";
    scope?: "mine" | "others" | "all";
    userId?: string | null;
    method?: "normal" | "vector" | "hybrid";
    size?: number;
    from?: number;
    logMeta?: { userId?: string | null; ip?: string | null; ua?: string | null } | null;
  }
): Promise<SearchHit[]> {
  // Use new chunked search system
  return searchDiariesSemanticChunked(userQuery, {
    userId: opts?.userId || "",
    mode: opts?.mode || "wide",
    scope: opts?.scope || "all",
    method: opts?.method || "normal",
    size: opts?.size || 20,
    from: opts?.from || 0,
  });
}
